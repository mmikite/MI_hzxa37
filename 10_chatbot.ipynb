{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmikite/MI_hzxa37/blob/main/10_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKGrc9Sm20E4"
      },
      "source": [
        "# Egy egyszerű chatbot készítése (NLTK használatával)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IinOrxpC20E9"
      },
      "source": [
        "## Szükséges könyvtárak importálása"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t7SxBfZ20E-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrIVWFt020FA"
      },
      "source": [
        "## Az NLTK letöltése és telepítése\n",
        "Az NLTK (Natural Language Toolkit) egy vezető platform az emberi nyelvi adatokkal dolgozó Python programok építéséhez. Könnyen használható interfészeket biztosít több mint 50 korpuszhoz és lexikai forráshoz, mint például a WordNet, valamint szövegfeldolgozó könyvtárak csomagját az osztályozáshoz, tokenizáláshoz, törzseléshez, címkézéshez, elemzéshez és szemantikai következtetéshez, valamint csomagolásokat ipari erősségű NLP-könyvtárakhoz."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYScPfEn20FA"
      },
      "outputs": [],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJfSlNW320FB"
      },
      "source": [
        "### NLTK csomagok telepítése"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiK2SHlK20FB"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) # for downloading packages\n",
        "nltk.download('punkt') # first-time use only\n",
        "nltk.download('wordnet') # first-time use only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJpxSfiD20FC"
      },
      "source": [
        "## A corpus beolvasása\n",
        "\n",
        "Példánkban a chatbotok Wikipedia oldalát fogjuk használni korpuszként; az oldal tartalmát belemásoltuk a \"chatbot.txt\" nevű szöveges fájlba. (Bármilyen tetszőleges korpuszt használhat.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Jl4Ivl20FC"
      },
      "outputs": [],
      "source": [
        "#lokális adatok esetén\n",
        "#f=open('chatbot.txt','r',errors = 'ignore')\n",
        "#raw=f.read()\n",
        "#raw = raw.lower()# converts to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImtrPQme20FD"
      },
      "outputs": [],
      "source": [
        "#távoli adatok esetén\n",
        "import urllib.request as urllib2\n",
        "\n",
        "raw = urllib2.urlopen(\"http://web.uni-corvinus.hu/~fszabina/data/chatbot.txt\").read().decode(\"latin-1\")\n",
        "raw = raw.lower()# converts to lowercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H6qfNVl20FD"
      },
      "source": [
        "A fő probléma a szöveges adatokkal az, hogy azok mind szöveges formátumúak (karakterláncok). A gépi tanulási algoritmusoknak azonban valamilyen numerikus jellemzővektorra van szükségük a feladat elvégzéséhez. Mielőtt azonban bármilyen NLP-projektbe belekezdenénk, a szöveget elő kell készítenünk, hogy ideális legyen a munkához. Az alapvető szöveg előfeldolgozás magában foglalja:\n",
        "\n",
        "* A teljes szöveg átalakítása **nagybetűssé** vagy **kisbetűssé**, hogy az algoritmus ne kezelje ugyanazokat a szavakat különböző esetekben különbözőként.\n",
        "\n",
        "* **Tokenization**: A tokenizálás csak a normál szöveges karakterláncok tokenek, azaz a kívánt szavak listájává történő átalakításának folyamatát írja le. A mondat tokenizáló használható a mondatok listájának megtalálására, a szó tokenizáló pedig a szavak listájának megtalálására a karakterláncokban.\n",
        "\n",
        "_Az NLTK adatcsomag tartalmaz egy előre betanított Punk tokenizálót az angol nyelvhez._\n",
        "\n",
        "\n",
        "* A **zajok** eltávolítása, azaz minden, ami nem egy szabványos szám vagy betű.\n",
        "* A **Stop szavak** eltávolítása. Néha néhány rendkívül gyakori szót, amelyeknek látszólag kevés értékük van a felhasználói igényeknek megfelelő dokumentumok kiválasztásában, teljesen kizárunk a szókincsből."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev8p53j620FE"
      },
      "source": [
        "## Tokenizálás"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwalgrwY20FE"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VptiBqkH20FF"
      },
      "source": [
        "## Előfeldolgozás\n",
        "\n",
        "Egy LemTokens nevű függvényt definiálunk, amely bemenetként elfogadja a tokeneket, és normalizált tokeneket ad vissza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bEKFCcB20FF"
      },
      "outputs": [],
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gML08I720FF"
      },
      "source": [
        "## Üdvözlés - kulcsszó illesztés\n",
        "\n",
        "Definiálunk egy függvényt a bot üdvözlésére, azaz ha a felhasználó bemenete egy üdvözlés, a bot egy üdvözlő választ ad vissza. Egyszerű kulcsszó illesztést fogunk használni."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxTaxnn720FG"
      },
      "outputs": [],
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNPi7rDQ20FG"
      },
      "source": [
        "## Válasz elkészítése\n",
        "\n",
        "### TF-IDF megközelítés\n",
        "a szavak gyakoriságának átméretezése aszerint, hogy milyen gyakran fordulnak elő az összes dokumentumban, így az olyan gyakori szavak pontszámai, mint a \"the\", amelyek szintén gyakoriak az összes dokumentumban, büntetést kapnak. A pontozásnak ezt a megközelítését Term Frequency-Inverse Document Frequency, röviden TF-IDF-nek nevezik, ahol:\n",
        "\n",
        "**Term Frequency: a szó gyakoriságának pontozása az aktuális dokumentumban.**\n",
        "\n",
        "```\n",
        "TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
        "```\n",
        "\n",
        "**Inverse Document Frequency: annak pontozása, hogy a szó mennyire ritka a dokumentumokban.**\n",
        "\n",
        "```\n",
        "IDF = 1+log(N/n), ahol, N a dokumentumok száma, n pedig azon dokumentumok száma, amelyekben a t kifejezés megjelent.\n",
        "```\n",
        "### Cosinus távolság (Cosine similarity)\n",
        "\n",
        "A Tf-idf súly az információkeresésben és a szövegbányászatban gyakran használt súly. Ez a súly egy statisztikai mérőszám, amelyet annak értékelésére használnak, hogy egy szó mennyire fontos egy dokumentumban egy gyűjteményben vagy korpuszban.\n",
        "\n",
        "```\n",
        "Cosine távolság (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
        "```\n",
        "ahol d1,d2 két nem nulla vektor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfSKslbg20FG"
      },
      "source": [
        "Ahhoz, hogy a robotunk válaszokat generáljon a bemeneti kérdésekre, a dokumentumhasonlóság fogalmát fogjuk használni. Definiálunk egy válasz függvényt, amely a felhasználó kijelentését egy vagy több ismert kulcsszóra keresi, és több lehetséges válasz közül egyet ad vissza. Ha nem talál a bemenetre illeszkedő kulcsszót, akkor a következő választ adja vissza:\" Sajnálom! Nem értem Önt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRe2a1F020FH"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    #A flatten() metódus egy többdimenziós NumPy tömböt egydimenziós tömbbé alakít.\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rivjYllB20FH"
      },
      "source": [
        "Végül, a felhasználó bevitelétől függően betápláljuk azokat a sorokat, amelyeket a botunknak mondania kell a beszélgetés megkezdésekor és befejezésekor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74e6LdJx20FI"
      },
      "outputs": [],
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \", end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}